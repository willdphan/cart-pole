{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/willdphan/cart-pole-q-learning/blob/master/Cart_Pole_V1_Q_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJNO7fmETnMX"
      },
      "source": [
        "# CartPole-v1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "FtCARfNuo-LK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wa1TqxuJTnMW"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "import numpy as np\n",
        "import time, math, random\n",
        "from typing import Tuple\n",
        "\n",
        "import gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8DhOfknTnMX",
        "outputId": "0fc521b2-8adf-48bc-e70e-5ff0ee377adc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('CartPole-v1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNuVWGGWTnMY"
      },
      "source": [
        "### Visualize Enviroment\n",
        "Visualize the eniroment/simulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRRcLn98TnMY",
        "outputId": "fcc68aa0-b391-48e5-ea9d-2bf70647c93c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
            "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/envs/classic_control/cartpole.py:179: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned terminated = True. You should always call 'reset()' once you receive 'terminated = True' -- any further steps are undefined behavior.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "policy = lambda obs: 1\n",
        "\n",
        "for _ in range(5):\n",
        "    obs = env.reset()\n",
        "    for _ in range(80):\n",
        "        actions = policy(obs)\n",
        "        obs, reward, done, info = env.step(actions)\n",
        "        env.render()\n",
        "        time.sleep(0.05)\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yl_qH7fRTnMY"
      },
      "source": [
        "Look at the docstring."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qPqATGwTnMY"
      },
      "source": [
        "### Hard Code Policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fb4TVAZiTnMZ"
      },
      "outputs": [],
      "source": [
        "# Simple policy function\n",
        "policy = lambda _,__,___, tip_velocity : int( tip_velocity > 0 )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--LEkFXmTnMZ"
      },
      "source": [
        "### Q-learning\n",
        "Convert Catpoles continues state space into discrete one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8RoYDqlTnMZ"
      },
      "outputs": [],
      "source": [
        "n_bins = ( 6 , 12 )\n",
        "lower_bounds = [ env.observation_space.low[2], -math.radians(50) ]\n",
        "upper_bounds = [ env.observation_space.high[2], math.radians(50) ]\n",
        "\n",
        "def discretizer( _ , __ , angle, pole_velocity ) -> Tuple[int,...]:\n",
        "    \"\"\"Convert continues state intro a discrete state\"\"\"\n",
        "    est = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform')\n",
        "    est.fit([lower_bounds, upper_bounds ])\n",
        "    return tuple(map(int,est.transform([[angle, pole_velocity]])[0]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4wECR5MTnMZ"
      },
      "source": [
        "Initialise the Q value table with zeros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjOKjrHYTnMZ"
      },
      "outputs": [],
      "source": [
        "Q_table = np.zeros(n_bins + (env.action_space.n,))\n",
        "Q_table.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2zzPZlZTnMa"
      },
      "source": [
        "Create a policy function,  uses the Q-table to and epsilon-greedly function to select the highest Q value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3x8j8j1wTnMa"
      },
      "outputs": [],
      "source": [
        "def policy( state : tuple ):\n",
        "    \"\"\"Choosing action based on epsilon-greedy policy\"\"\"\n",
        "    return np.argmax(Q_table[state])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3y7SbVgtTnMa"
      },
      "source": [
        "Update function that updates the Q-vale and the state-action pair."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PGWf965TnMa"
      },
      "outputs": [],
      "source": [
        "def new_Q_value( reward : float ,  new_state : tuple , discount_factor=1 ) -> float:\n",
        "    \"\"\"Temperal diffrence for updating Q-value of state-action pair\"\"\"\n",
        "    future_optimal_value = np.max(Q_table[new_state])\n",
        "    learned_value = reward + discount_factor * future_optimal_value\n",
        "    return learned_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EjUlnekTnMa"
      },
      "source": [
        "Get the Decaying LR to enforce adaptive learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKVg8_f8TnMa"
      },
      "outputs": [],
      "source": [
        "# Adaptive learning of Learning Rate\n",
        "def learning_rate(n : int , min_rate=0.01 ) -> float  :\n",
        "    \"\"\"Decaying learning rate\"\"\"\n",
        "    return max(min_rate, min(1.0, 1.0 - math.log10((n + 1) / 25)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1mYUVdVTnMa"
      },
      "source": [
        "Decaying exploration rate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMR43Z-JTnMa"
      },
      "outputs": [],
      "source": [
        "def exploration_rate(n : int, min_rate= 0.1 ) -> float :\n",
        "    \"\"\"Decaying exploration rate\"\"\"\n",
        "    return max(min_rate, min(1, 1.0 - math.log10((n  + 1) / 25)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ct-tAIMBTnMb"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQZrK6otTnMb"
      },
      "outputs": [],
      "source": [
        "\n",
        "n_episodes = 10000\n",
        "for e in range(n_episodes):\n",
        "\n",
        "    # Siscretize state into buckets\n",
        "    current_state, done = discretizer(*env.reset()), False\n",
        "\n",
        "    while done==False:\n",
        "\n",
        "        # policy action\n",
        "        action = policy(current_state) # exploit\n",
        "\n",
        "        # insert random action\n",
        "        if np.random.random() < exploration_rate(e) :\n",
        "            action = env.action_space.sample() # explore\n",
        "\n",
        "        # increment enviroment\n",
        "        obs, reward, done, _ = env.step(action)\n",
        "        new_state = discretizer(*obs)\n",
        "\n",
        "        # Update Q-Table\n",
        "        lr = learning_rate(e)\n",
        "        learnt_value = new_Q_value(reward , new_state )\n",
        "        old_value = Q_table[current_state][action]\n",
        "        Q_table[current_state][action] = (1-lr)*old_value + lr*learnt_value\n",
        "\n",
        "        current_state = new_state\n",
        "\n",
        "        # Render the cartpole environment\n",
        "        env.render()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}